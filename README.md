# Ultrasonic Perception Visualization for Mobile Robots

This repository demonstrates a **clear and interpretable ultrasonic perception layer** for a mobile robot.  
It visualizes how **24 ultrasonic sensors** perceive obstacles around the robot and how these measurements are transformed into a **top-down world view with robot motion and heading**.

The goal of this project is **perception understanding and debugging**.
---

## ğŸ“Œ What This Project Shows

This project focuses on **how raw ultrasonic sensor data becomes spatial awareness**.

Specifically, it demonstrates:

- Real-time visualization of **24 ultrasonic sensors** arranged around a robot
- Conversion of sensor readings from **polar space â†’ world coordinates**
- Obstacle detection based on a configurable distance threshold
- Robot **trajectory, heading, and motion context**
- Frame-by-frame animation to observe perception over time

This is an essential **first layer** of a Physical AI / Robotics Perception Stack.

---

## ğŸ§  Why Ultrasonic Sensors?

Ultrasonic sensors are:

- Low-cost
- Short-range
- Widely used for **collision avoidance and safety**

They are **not high-resolution like LiDAR**, and therefore should **not** look like dense point clouds.
 
---

## ğŸ“Š Visualization Overview

### 1ï¸âƒ£ Polar Sensor View (Left)

- Each spike corresponds to one ultrasonic sensor (US1â€“US24)
- Angles match physical sensor placement
- Distance indicates obstacle proximity
- Red highlights show **near obstacles**

---

### 2ï¸âƒ£ Top-Down World View (Right)

- Robot position and heading arrow
- Past trajectory (path history)
- Ultrasonic points projected into global coordinates
- Highlighted obstacles within detection range
- Circular detection boundary for safety margin

---

## ğŸ–¼ï¸ Example Result

Below is an example frame from the animation generated by this code:

- **Left:** Polar ultrasonic perception
- **Right:** Top-down spatial map with robot heading and obstacles

> ğŸ“Œ *Note:* Visualizations are best viewed by **running the code locally** to see the full animation and interaction.

![Ultrasonic Perception Result] 
<img width="1400" height="700" alt="Figure_1" src="https://github.com/user-attachments/assets/d80bfe7a-ffc4-41da-a751-fed501bd329a" />

---

## ğŸ“ Dataset Format

The input dataset is a CSV file with:

- **24 ultrasonic sensor readings per frame**
- A **movement class label** per frame

Example columns:
US1, US2, ..., US24, Class

Example movement classes:
- `Move-Forward`
- `Slight-Left-Turn`
- `Slight-Right-Turn`
- `Sharp-Right-Turn`

---

## ğŸ› ï¸ Libraries Used

- **Python**
- **Pandas** â€“ data loading and cleaning
- **NumPy** â€“ geometry and math
- **Matplotlib** â€“ polar plots, animations, and maps

All libraries are standard and lightweight.

---

## ğŸ§© Where This Fits in a Perception Stack

This project represents:

[ Sensors ]
â†“
[ Ultrasonic Perception ] â† (this repository)
â†“
[ LiDAR / Camera Perception ]
â†“
[ Sensor Fusion ]
â†“
[ Planning & Control ]


It validates **sensor geometry, coordinate transforms, and obstacle logic** before scaling to more complex sensors.

---

## â–¶ï¸ How to Run

1. Place your dataset file:

2. Install dependencies:
```bash
pip install numpy pandas matplotlib


